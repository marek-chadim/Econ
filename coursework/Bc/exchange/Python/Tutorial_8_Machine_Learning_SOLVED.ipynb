{"cells":[{"cell_type":"markdown","id":"3fa3394b","metadata":{"id":"3fa3394b"},"source":["# Machine Learning with Python"]},{"cell_type":"markdown","id":"6b66195a","metadata":{"id":"6b66195a"},"source":["## Brief history of Machine Learning\n","\n","Machine learning (ML), a subset of artificial intelligence (AI), has a long history from the dawn of computing. The first mathematical model of an artificial neuron was developed in the 1940s and 1950s, laying the groundwork for ML. Turing, Minsky, and McCarthy were early AI pioneers who created early AI systems. With Samuel's checkers-playing programme and Rosenblatt's perceptron method, ML gained traction.\n","\n","Expert systems, rule-based systems, and neural network learning processes were the focus of research in the 1970s and 1980s. By inventing Support Vector Machines and popularising data mining in the 1990s, ML switched towards data-driven techniques. Deep learning techniques such as convolutional and recurrent neural networks enabled rapid AI developments in the 2010s.\n","\n","Today, machine learning is a fast-emerging field with applications across industries, emphasising its significance in the broader landscape of artificial intelligence and its potential to revolutionise different aspects of our life.\n","\n","\n","## Definition of Machine Learning\n","Machine learning is a subset of artificial intelligence that allows computers to learn from data and make predictions or decisions without being explicitly programmed.\n","\n","## Importance and Applications of Machine Learning in Daily Life\n","Machine learning has a wide range of applications in our daily lives, including:\n","- Email spam filtering\n","- Product recommendations on e-commerce websites\n","- Personalized content on streaming platforms\n","- Voice recognition and assistants\n","- Fraud detection in financial transactions\n","\n","# Types of Machine Learning\n","\n","## Supervised Learning\n","Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, which includes both input features and the correct output. There are two main types of supervised learning:\n","\n",">### Classification\n","Classification is the process of predicting a categorical output or class. Examples include spam detection, image recognition, and medical diagnosis.\n","\n",">### Regression\n","Regression involves predicting a continuous output or quantity. Examples include housing price prediction, stock market analysis, and sales forecasting.\n","\n","<b>Personalized ads on platforms like Instagram and Facebook are examples of machine learning in action. Specifically, they can be considered as applications of supervised learning, particularly classification and regression tasks.</b>\n","\n","## Unsupervised Learning\n","Unsupervised learning is a type of machine learning where the algorithm is trained on an unlabeled dataset, meaning it doesn't have access to the correct output. There are two main types of unsupervised learning:\n","\n",">### Clustering\n","Clustering is the process of grouping similar data points based on their features. Examples include customer segmentation, social network analysis, and anomaly detection.\n","\n",">### Dimensionality Reduction\n","Dimensionality reduction involves reducing the number of features in the dataset while preserving its structure. Examples include Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE).\n","\n","## Reinforcement Learning\n","Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. Examples include robotics, game playing, and autonomous vehicles.\n","\n","The classic example of reinforcement learning: In October 2015, the distributed version of AlphaGo defeated the European Go champion Fan Hui. You can find more about it here:\n",">https://www.deepmind.com/research/highlighted-research/alphago\n","\n",">https://en.wikipedia.org/wiki/AlphaGo#:~:text=Match%20against%20Fan%20Hui,-Main%20article%3A%20AlphaGo&text=In%20October%202015%2C%20the%20distributed,full%2Dsized%20board%20without%20handicap.\n","\n","\n","# Basic Terminology\n","\n",">## Data: Features and Labels\n","- **Features**: The independent variables or input data used to train a machine learning model.\n","- **Labels**: The dependent variables or output data that the model tries to predict.\n","\n",">## Model\n","A model is an algorithm that learns from the input data (features) to make predictions or decisions.\n","\n",">## Training\n","Training is the process of teaching a model using the input data (features) and the corresponding output data (labels).\n","\n",">## Prediction\n","Prediction is the process of making educated guesses or estimates based on the trained model and new, unseen input data."]},{"cell_type":"markdown","id":"06a9c815","metadata":{"id":"06a9c815"},"source":["### Linear Regression\n","\n","Linear regression is a simple machine learning algorithm used to model the relationship between a target variable (output) and one or more input features. It assumes that the relationship between the target and input features is linear. It's commonly used for predicting numerical values."]},{"cell_type":"code","execution_count":null,"id":"1160faa1","metadata":{"id":"1160faa1"},"outputs":[],"source":["import numpy as np\n","from sklearn.linear_model import LinearRegression\n","\n","# Generate house sizes (in square meters) and prices\n","house_sizes = np.arange(20, 70, 1).reshape(-1, 1)\n","prices = 490 + house_sizes * 10\n","'''\n","This line of code generates an array of house prices using a simple linear relationship with the house sizes. \n","The base price of a house is set to 500, and the price increases by 10 for every additional square meter of the house size. \n","This relationship is expressed as: price = 490 + (10 * house_size).\n","'''\n","\n","# Create and train the linear regression model\n","model = LinearRegression()\n","model.fit(house_sizes, prices)\n","\n","# Predict the price of a house with an area in square meters\n","predicted_price = model.predict([[17]])\n","print(\"Predicted price of an 80 sqm house:\", predicted_price[0][0])"]},{"cell_type":"markdown","id":"7d6e09a7","metadata":{"id":"7d6e09a7"},"source":["### Decision Tree Classifier\n","A Decision Tree Classifier is a type of machine learning model used for making decisions or predictions based on data. \n","In layman terms, it can be thought of as a flowchart or a tree-like structure that helps in making decisions by following a \n","series of simple rules or questions."]},{"cell_type":"code","execution_count":null,"id":"c8b00783","metadata":{"id":"c8b00783"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn import tree\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n","                       columns=iris['feature_names'] + ['target'])\n","\n","# Prepare the features (X) and target (y) arrays\n","X, y = iris_df.iloc[:, :4].values, iris_df.iloc[:, -1].values\n","\n","# Split the dataset into training (80%) and testing (20%) sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=18)\n","\n","# Train a decision tree classifier on the training set\n","dt = tree.DecisionTreeClassifier()\n","\n","dt.fit(X_train, y_train)\n","\n","# Make a prediction on a randomly chosen sample from the dataset\n","random_sample = iris_df.sample(1)\n","random_sample_x = random_sample.iloc[0, :4].values\n","\n","# Predict the class of the random sample\n","prediction = dt.predict(random_sample_x.reshape(1, -1))\n","print(prediction)"]},{"cell_type":"markdown","id":"5048808b","metadata":{"id":"5048808b"},"source":["The output is the predicted class label for the randomly chosen iris flower sample. The class label is an integer that represents one of the three iris species in the dataset:\n","\n","- 0: Iris setosa\n","- 1: Iris versicolor\n","- 2: Iris virginica\n","\n","In this case, the output [1,] indicates that the model predicts the randomly selected iris flower to be of the species Iris versicolor. Keep in mind that since the code is using a random sample, the output may be different each time the code is executed, and it may predict a different class for another random sample."]},{"cell_type":"markdown","id":"08d421f9","metadata":{"id":"08d421f9"},"source":["The `np.c_[]` function is used to concatenate the feature data (iris['data']) and target data (iris['target']) column-wise. This means that the target column is added as the last column after the feature columns. The result is a two-dimensional NumPy array that will be used as the data for the DataFrame.\n","\n","##### What is Iris dataset?\n","The Iris dataset, also known as the Fisher's Iris dataset, is a classic and widely used dataset in the field of machine learning and statistics. The dataset consists of 150 samples of iris flowers, equally divided into three classes representing three different species of iris: Iris setosa, Iris versicolor, and Iris virginica. For each sample, there are four features (or attributes) measured in centimeters:\n","\n","1. Sepal length\n","2. Sepal width\n","3. Petal length\n","4. Petal width\n","\n","The main objective when working with the Iris dataset is to develop a classification model that can accurately predict the species of an iris flower based on its sepal and petal measurements. Due to its simplicity and clear structure, the Iris dataset is often used as an introductory dataset for teaching machine learning techniques and classification algorithms.\n","\n","##### Splitting the dataset\n","Splitting the dataset into separate training and testing sets is essential for evaluating the performance and generalization ability of a machine learning model. By training the model on one subset of the data and testing it on another unseen subset, we can estimate how well the model will perform on new, unseen data.\n","\n","The choice of splitting ratio, such as 80-20, 70-30, or 60-40, depends on the size of the dataset and the problem at hand. There is no strict rule for choosing the exact ratio, but some general guidelines can be followed:\n","\n","- If the dataset is large enough, an 80-20 or 70-30 split can provide a good balance between the amount of data used for training the model and the amount reserved for testing its performance.\n","- If the dataset is relatively small, a higher proportion of the data should be allocated to the training set to ensure that the model has enough examples to learn from. In such cases, a 90-10 or 85-15 split might be more appropriate.\n","- When dealing with imbalanced datasets or situations where the model's performance on a specific class is of utmost importance, techniques like stratified sampling or cross-validation can be used to ensure a better distribution of classes in both training and testing sets."]},{"cell_type":"markdown","id":"bd1e6150","metadata":{"id":"bd1e6150"},"source":["### Logistic regression\n","\n","Logistic regression is a way to predict whether something belongs to one group or another based on certain features. Imagine you have a basket of apples and oranges, and you want to identify the fruit type based on its color and size. Logistic regression helps you find the best way to separate the apples from the oranges using these features.\n","\n","In simple terms, logistic regression takes information about an item (like its color and size) and calculates the probability of it belonging to a specific group (e.g., apple or orange). It does this by learning from examples where the right answer is already known. Once the model is trained, it can be used to predict the group membership of new, unseen items."]},{"cell_type":"code","execution_count":null,"id":"393ae02f","metadata":{"id":"393ae02f"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","import pandas as pd\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n","                       columns=iris['feature_names'] + ['target'])\n","\n","# Train a logistic regression model on the training set\n","logreg = LogisticRegression(max_iter=300, random_state=0)\n","logreg.fit(X_train, y_train)\n","\n","# Select a random sample from the dataset\n","random_sample = iris_df.sample(1)\n","random_sample_x = random_sample.iloc[0, :4].values\n","\n","# Predict the class of the random sample using logistic regression\n","pred_log = logreg.predict(random_sample_x.reshape(1, -1))\n","print(pred_log)\n","\n","# Predict the class probabilities of the random sample using logistic regression\n","pred_proba = logreg.predict_proba(random_sample_x.reshape(1, -1))\n","print(pred_proba)"]},{"cell_type":"markdown","id":"1d8a2608","metadata":{"id":"1d8a2608"},"source":["[2.]: This is the predicted class label for the randomly chosen iris flower sample. \n","\n","This is a two-dimensional array containing the predicted class probabilities for the random sample. Each value in the array represents the probability of the sample belonging to a specific class, according to the logistic regression model. The values in the array sum up to 1.\n","\n","In this example, the class probabilities are:\n","Iris setosa (class 0): 0.00070041\n","Iris versicolor (class 1): 0.49433337\n","Iris virginica (class 2): 0.50496622\n","\n","The model predicts that the random sample has a very low probability of being Iris setosa, a slightly higher probability of being Iris versicolor, and the highest probability of being Iris virginica. As expected, the class with the highest probability (Iris virginica) corresponds to the predicted class label [2.]."]},{"cell_type":"markdown","id":"f9965a79","metadata":{"id":"f9965a79"},"source":["## Accuracy"]},{"cell_type":"code","execution_count":null,"id":"eb45c7c5","metadata":{"id":"eb45c7c5"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","# Predict class labels for the test and train sets\n","pred_dt = dt.predict(X_test)\n","pred_dt_train = dt.predict(X_train)\n","pred_logreg_train = logreg.predict(X_train)\n","pred_logreg = logreg.predict(X_test)\n","\n","# Calculate the accuracy scores for the decision tree and logistic regression models\n","dt_acc = accuracy_score(y_test, pred_dt)\n","logreg_acc = accuracy_score(y_test, pred_logreg)\n","\n","# Print the accuracy scores\n","print(f\"DT accuracy: {dt_acc}\\nLogistic Regression accuracy: {logreg_acc}\")"]},{"cell_type":"markdown","id":"e830e1cf","metadata":{"id":"e830e1cf"},"source":["Accuracy score is a commonly used metric to evaluate the performance of classification models.\n","\n","An accuracy score of 0.95 means that the model correctly predicts the class labels for 95% of the samples in the test set. A higher accuracy score indicates better performance, but it's important to keep in mind that accuracy can be misleading in cases where the dataset is imbalanced or when the model's performance on specific classes is more important. In such cases, other evaluation metrics like precision, recall, or F1 score might be more appropriate."]},{"cell_type":"markdown","id":"fe27a893","metadata":{"id":"fe27a893"},"source":["Recall (sensitivity or true positive rate) measures the proportion of actual positive instances that were correctly predicted by the model. A high recall indicates that the model is good at identifying positive instances.\n","\n","Precision (positive predictive value) measures the proportion of true positive instances among the instances predicted as positive by the model. A high precision indicates that the model is good at not labeling negative instances as positive.\n","\n","F1 score is the harmonic mean of precision and recall, and it balances both metrics. It's a good single-number summary of a model's performance, especially when dealing with imbalanced datasets or when both false positives and false negatives are important to consider."]},{"cell_type":"code","execution_count":null,"id":"8c51fad7","metadata":{"id":"8c51fad7"},"outputs":[],"source":["from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n","\n","# Calculate recall, precision, and F1 score for the decision tree model\n","recall = recall_score(y_test, pred_dt, average=\"macro\")\n","prec_score = precision_score(y_test, pred_dt, average=\"macro\")\n","f1 = f1_score(y_test, pred_dt, average=\"macro\")\n","\n","# Print the recall, precision, and F1 score\n","print(f\"DT precision, recall, and F1: {recall, prec_score, f1}\")\n","\n","# Print the classification report for the decision tree model\n","print(\"----Decision Tree----\")\n","print(classification_report(y_test, pred_dt))"]},{"cell_type":"markdown","id":"1b1f8860","metadata":{"id":"1b1f8860"},"source":["This code snippet calculates the precision, recall, and F1 score for the decision tree model on the test set and then prints a summary of these classification metrics using the classification_report function."]},{"cell_type":"code","execution_count":null,"id":"6d61f1f6","metadata":{"id":"6d61f1f6"},"outputs":[],"source":["print(\"Logistic Regression\")\n","print(classification_report(y_test, pred_logreg))"]},{"cell_type":"code","execution_count":null,"id":"56ab4391","metadata":{"id":"56ab4391"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","y_pred = dt.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","\n","print(\"Decision Tree\")\n","cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n","cmd.plot()"]},{"cell_type":"code","execution_count":null,"id":"da19c17c","metadata":{"id":"da19c17c"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n","                       columns=iris['feature_names'] + ['target'])\n","\n","# Prepare the features (X) and target (y) arrays\n","X, y = iris_df.iloc[:, :4].values, iris_df.iloc[:, -1].values\n","\n","# Split the dataset into training (80%) and testing (20%) sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=18)\n","\n","# Train a logistic regression model on the training set\n","logreg = LogisticRegression(max_iter=300, random_state=0)\n","logreg.fit(X_train, y_train)\n","\n","# Predict the labels for the test set\n","y_pred = logreg.predict(X_test)\n","\n","# Print the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Plot the confusion matrix using ConfusionMatrixDisplay\n","print(\"Logistic Regression\")\n","cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n","cmd.plot()"]},{"cell_type":"markdown","id":"7e2154d9","metadata":{"id":"7e2154d9"},"source":["The confusion matrix provides a visual representation of the model's performance, making it easier to see where the model is making correct predictions and where it is making mistakes. The diagonal elements in the confusion matrix represent the number of correct predictions, while the off-diagonal elements represent the number of incorrect predictions. A perfect model would have all its predictions on the diagonal, resulting in a confusion matrix with zero values in all off-diagonal elements."]},{"cell_type":"markdown","source":["# Task 1\n","\n","Create a model using Logistic Regression classifier from diabetes.csv data. Choose the Outcome column (defines if the patient has diabetes or not) as the target variable. To do that, compute the following:\n","a) Create train and test splits. The train split should contain 80% of the data, and the test split, 20%.\n","b) Fit the logistic regression model with data\n","\n","Hint: Explore the data first"],"metadata":{"id":"hU2QZImNkVIq"},"id":"hU2QZImNkVIq"},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","# Read the dataset\n","diabetes_data = pd.read_csv('diabetes.csv')\n","\n","# Separate features and target\n","X = diabetes_data.drop('Outcome', axis=1)\n","y = diabetes_data['Outcome']\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and fit the logistic regression model\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Check the model's accuracy on the test set\n","acc = model.score(X_test, y_test)\n","print('Accuracy:', acc)"],"metadata":{"id":"ZhjtNanRkdii"},"id":"ZhjtNanRkdii","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"6bca57cf","metadata":{"id":"6bca57cf"},"source":["# Task 2:\n","Use the logistic regression model that you created from diabetes.csv at Task 1 for answering following questions:\n","\n","- a) Provide prediction for the patients with diabetes\n","- b) Evaluate your model using Confusion Matrix\n","- c) Evaluate your model using accuracy, precision, and recall."]},{"cell_type":"code","execution_count":null,"id":"e2e9cabf","metadata":{"id":"e2e9cabf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n","\n","# Load the diabetes dataset\n","diabetes_df = pd.read_csv('diabetes.csv')\n","\n","# Preprocess the data\n","X = diabetes_df.drop(columns=['Outcome']).values\n","y = diabetes_df['Outcome'].values\n","\n","# Split the data into training (70%) and testing (30%) sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a logistic regression model on the training set\n","logreg = LogisticRegression(max_iter=1000, random_state=0)\n","logreg.fit(X_train, y_train)\n","\n","# Provide prediction for the patients with diabetes\n","pred = logreg.predict(X_test)\n","print(pred)\n","\n","# Evaluate the model using confusion matrix\n","cm = confusion_matrix(y_test, pred)\n","print(cm)\n","\n","# Evaluate the model using accuracy, precision, and recall\n","acc = accuracy_score(y_test, pred)\n","prec = precision_score(y_test, pred)\n","rec = recall_score(y_test, pred)\n","print('Accuracy:', acc)\n","print('Precision:', prec)\n","print('Recall:', rec)"]},{"cell_type":"markdown","id":"ae049fa9","metadata":{"id":"ae049fa9"},"source":["## Cross validation\n","Cross-validation is a technique used to evaluate the performance of a machine learning model by testing its ability to make predictions on new, unseen data. It helps to ensure that the model is not just memorizing the training data but is actually learning to generalize from it.\n","\n","The main idea behind cross-validation is to split the available data into several smaller sets, called \"folds.\" The model is then trained on some of these folds and tested on the remaining fold(s). This process is repeated multiple times, with different folds being used for testing each time. Finally, the results from each round are combined to provide an overall estimate of the model's performance.\n","\n","By doing this, cross-validation helps to get a better idea of how well the model will perform on new data, while still making the most of the available data for training. It also helps to avoid overfitting, which is when the model becomes too specialized in the training data and performs poorly on new, unseen data."]},{"cell_type":"code","execution_count":null,"id":"266caa7a","metadata":{"id":"266caa7a"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","\n","# Create the decision tree and logistic regression models\n","dt = tree.DecisionTreeClassifier()\n","logreg = LogisticRegression(random_state=18)\n","\n","# Perform 5-fold cross-validation on the decision tree model\n","dt_cv = cross_val_score(dt, X, y, scoring=\"accuracy\", cv=5)\n","print(\"Decision Tree accuracy scores for each fold:\", dt_cv)\n","\n","# Perform 5-fold cross-validation on the logistic regression model\n","logreg_cv = cross_val_score(logreg, X, y, scoring=\"accuracy\", cv=5)\n","print(\"\\nLogistic Regression accuracy scores for each fold:\", logreg_cv)"]},{"cell_type":"markdown","id":"8b7fe603","metadata":{"id":"8b7fe603"},"source":["The code snippet performs 5-fold cross-validation for both the decision tree and logistic regression models on the iris dataset. It calculates the accuracy score for each fold and then prints the scores for both models."]},{"cell_type":"markdown","id":"a1dc3869","metadata":{"id":"a1dc3869"},"source":["# Task 3\n","Implemented 5 fold cross-validation with LogisticRegression on a sklearn dataset that is called load_digits. You can randomly select your target variable."]},{"cell_type":"markdown","id":"4a59e3a9","metadata":{"id":"4a59e3a9"},"source":["# Clustering\n","\n","K-means clustering is a popular clustering algorithm that aims to partition the data into K clusters, where each data point belongs to the cluster with the nearest mean (centroid). The algorithm iteratively refines the centroids until they are stable, meaning the data points' assignments to clusters don't change anymore."]},{"cell_type":"markdown","id":"0ef04775","metadata":{"id":"0ef04775"},"source":["### Basic concept"]},{"cell_type":"code","execution_count":null,"id":"1f0c64c3","metadata":{"id":"1f0c64c3"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","\n","# Generate some random data\n","np.random.seed(42)\n","X = np.random.rand(50, 2)\n","\n","# Perform k-means clustering with k=3\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","kmeans.fit(X)\n","\n","# Plot the clusters\n","plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n","plt.show()"]},{"cell_type":"markdown","id":"bcfd38ad","metadata":{"id":"bcfd38ad"},"source":["### A more complex example"]},{"cell_type":"code","execution_count":null,"id":"6d7767af","metadata":{"id":"6d7767af"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the dataset\n","df = pd.read_csv(\"datasets/countries.csv\")\n","\n","# Preprocess data\n","scaler = StandardScaler()\n","X = scaler.fit_transform(df[['latitude', 'longitude']])\n","\n","# K-Means clustering\n","n_clusters = 7\n","kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n","df['Cluster'] = kmeans.fit_predict(X)\n","\n","# Display clusters\n","for i in range(n_clusters):\n","    print(f\"Cluster {i}:\")\n","    print(df[df['Cluster'] == i]['country'].tolist())\n","    print()\n"]},{"cell_type":"code","execution_count":null,"id":"0481a86d","metadata":{"id":"0481a86d"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Create a scatter plot with different colors for each cluster\n","colors = plt.cm.get_cmap('viridis', n_clusters)\n","fig, ax = plt.subplots(figsize=(12, 8))\n","\n","for i in range(n_clusters):\n","    cluster_data = df[df['Cluster'] == i]\n","    ax.scatter(cluster_data['longitude'], cluster_data['latitude'], c=[colors(i)], label=f'Cluster {i}', alpha=0.6, edgecolors='w', s=100)\n","\n","ax.set_xlabel('Longitude')\n","ax.set_ylabel('Latitude')\n","ax.set_title('Country Clusters')\n","ax.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"874f76a3","metadata":{"id":"874f76a3"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[],"collapsed_sections":["6b66195a","06a9c815","7d6e09a7","08d421f9","bd1e6150","4a59e3a9"]}},"nbformat":4,"nbformat_minor":5}