summary(lm.fit)
###
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
###
library(car)
vif(lm.fit)
###
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
###
lm.fit1 <- update(lm.fit, ~ . - age)
###
summary(lm(medv ~ lstat * age, data = Boston))
###
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
###
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
###
par(mfrow = c(2, 2))
plot(lm.fit2)
###
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)
###
summary(lm(medv ~ log(rm), data = Boston))
###
head(Carseats)
###
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age,
data = Carseats)
summary(lm.fit)
###
attach(Carseats)
contrasts(ShelveLoc)
###
LoadLibraries
LoadLibraries()
###
LoadLibraries <- function() {
library(ISLR2)
library(MASS)
print("The libraries have been loaded.")
}
###
LoadLibraries
###
LoadLibraries()
###
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
###
lfit <- lm(Salary ~ ., data = Gitters[-testid, ])
lpred <- predict(lfit, Gitters[testid, ])
with(Gitters[testid, ], mean(abs(lpred - Salary)))
###
x <- scale(model.matrix(Salary ~ . - 1, data = Gitters))
y <- Gitters$Salary
###
library(glmnet)
knitr::opts_chunk$set(error = TRUE)
library(ISLR2)
names(BrainCancer)
attach(BrainCancer)
table(sex)
table(diagnosis)
table(status)
library(survival)
fit.surv <- survfit(Surv(time, status) ~ 1)
plot(fit.surv, xlab = "Months",
ylab = "Estimated Probability of Survival")
fit.sex <- survfit(Surv(time, status) ~ sex)
plot(fit.sex, xlab = "Months",
ylab = "Estimated Probability of Survival", col = c(2,4))
legend("bottomleft", levels(sex), col = c(2,4), lty = 1)
logrank.test <- survdiff(Surv(time, status) ~ sex)
logrank.test
fit.cox <- coxph(Surv(time, status) ~ sex)
summary(fit.cox)
summary(fit.cox)$logtest[1]
summary(fit.cox)$waldtest[1]
summary(fit.cox)$sctest[1]
logrank.test$chisq
fit.all <- coxph(
Surv(time, status) ~ sex + diagnosis + loc + ki + gtv +
stereo)
fit.all
modaldata <- data.frame(
diagnosis = levels(diagnosis),
sex = rep("Female", 4),
loc = rep("Supratentorial", 4),
ki = rep(mean(ki), 4),
gtv = rep(mean(gtv), 4),
stereo = rep("SRT", 4)
)
survplots <- survfit(fit.all, newdata = modaldata)
plot(survplots, xlab = "Months",
ylab = "Survival Probability", col = 2:5)
legend("bottomleft", levels(diagnosis), col = 2:5, lty = 1)
fit.posres <- survfit(
Surv(time, status) ~ posres, data = Publication
)
plot(fit.posres, xlab = "Months",
ylab = "Probability of Not Being Published", col = 3:4)
legend("topright", c("Negative Result", "Positive Result"),
col = 3:4, lty = 1)
fit.pub <- coxph(Surv(time, status) ~ posres,
data = Publication)
fit.pub
logrank.test <- survdiff(Surv(time, status) ~ posres,
data = Publication)
logrank.test
fit.pub2 <- coxph(Surv(time, status) ~ . - mech,
data = Publication)
fit.pub2
set.seed(4)
N <- 2000
Operators <- sample(5:15, N, replace = T)
Center <- sample(c("A", "B", "C"), N, replace = T)
Time <- sample(c("Morn.", "After.", "Even."), N, replace = T)
X <- model.matrix( ~ Operators + Center + Time)[, -1]
X[1:5, ]
true.beta <- c(0.04, -0.3, 0, 0.2, -0.2)
h.fn <- function(x) return(0.00001 * x)
library(coxed)
queuing <- sim.survdata(N = N, T = 1000, X = X,
beta = true.beta, hazard.fun = h.fn)
names(queuing)
head(queuing$data)
mean(queuing$data$failed)
fit.Center <- survfit(Surv(y, failed) ~ Center,
data = queuing$data)
plot(fit.Center, xlab = "Seconds",
ylab = "Probability of Still Being on Hold",
col = c(2, 4, 5))
legend("topright",
c("Call Center A", "Call Center B", "Call Center C"),
col = c(2, 4, 5), lty = 1)
fit.Time <- survfit(Surv(y, failed) ~ Time,
data = queuing$data)
plot(fit.Time, xlab = "Seconds",
ylab = "Probability of Still Being on Hold",
col = c(2, 4, 5))
legend("topright", c("Morning", "Afternoon", "Evening"),
col = c(5, 2, 4), lty = 1)
survdiff(Surv(y, failed) ~ Center, data = queuing$data)
survdiff(Surv(y, failed) ~ Time, data = queuing$data)
fit.queuing <- coxph(Surv(y, failed) ~ .,
data = queuing$data)
fit.queuing
knitr::opts_chunk$set(error = TRUE)
states <- row.names(USArrests)
states
names(USArrests)
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
pr.out <- prcomp(USArrests, scale = TRUE)
names(pr.out)
pr.out$center
pr.out$scale
pr.out$rotation
dim(pr.out$x)
biplot(pr.out, scale = 0)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)
pr.out$sdev
pr.var <- pr.out$sdev^2
pr.var
pve <- pr.var / sum(pr.var)
pve
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
a <- c(1, 2, 8, -3)
cumsum(a)
X <- data.matrix(scale(USArrests))
pcob <- prcomp(X)
summary(pcob)
sX <- svd(X)
names(sX)
round(sX$v, 3)
pcob$rotation
t(sX$d * t(sX$u))
pcob$x
nomit <- 20
set.seed(15)
ina <- sample(seq(50), nomit)
inb <- sample(1:4, nomit, replace = TRUE)
Xna <- X
index.na <- cbind(ina, inb)
Xna[index.na] <- NA
fit.svd <- function(X, M = 1) {
svdob <- svd(X)
with(svdob,
u[, 1:M, drop = FALSE] %*%
(d[1:M] * t(v[, 1:M, drop = FALSE]))
)
}
Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]
thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)
while(rel_err > thresh) {
iter <- iter + 1
# Step 2(a)
Xapp <- fit.svd(Xhat, M = 1)
# Step 2(b)
Xhat[ismiss] <- Xapp[ismiss]
# Step 2(c)
mss <- mean(((Xna - Xapp)[!ismiss])^2)
rel_err <- (mssold - mss) / mss0
mssold <- mss
cat("Iter:", iter, "MSS:", mss,
"Rel. Err:", rel_err, "\n")
}
cor(Xapp[ismiss], X[ismiss])
set.seed(2)
x <- matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
km.out <- kmeans(x, 2, nstart = 20)
km.out$cluster
#par(mfrow = c(1, 2))
plot(x, col = (km.out$cluster + 1),
main = "K-Means Clustering Results with K = 2",
xlab = "", ylab = "", pch = 20, cex = 2)
set.seed(4)
km.out <- kmeans(x, 3, nstart = 20)
km.out
plot(x, col = (km.out$cluster + 1),
main = "K-Means Clustering Results with K = 3",
xlab = "", ylab = "", pch = 20, cex = 2)
set.seed(4)
km.out <- kmeans(x, 3, nstart = 1)
km.out$tot.withinss
km.out <- kmeans(x, 3, nstart = 20)
km.out$tot.withinss
hc.complete <- hclust(dist(x), method = "complete")
hc.average <- hclust(dist(x), method = "average")
hc.single <- hclust(dist(x), method = "single")
par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)
plot(hc.single, main = "Single Linkage",
xlab = "", sub = "", cex = .9)
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
cutree(hc.single, 4)
xsc <- scale(x)
plot(hclust(dist(xsc), method = "complete"),
main = "Hierarchical Clustering with Scaled Features")
x <- matrix(rnorm(30 * 3), ncol = 3)
dd <- as.dist(1 - cor(t(x)))
plot(hclust(dd, method = "complete"),
main = "Complete Linkage with Correlation-Based Distance",
xlab = "", sub = "")
library(ISLR2)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
pr.out <- prcomp(nci.data, scale = TRUE)
Cols <- function(vec) {
cols <- rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))])
}
par(mfrow = c(1, 2))
plot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19,
xlab = "Z1", ylab = "Z2")
plot(pr.out$x[, c(1, 3)], col = Cols(nci.labs), pch = 19,
xlab = "Z1", ylab = "Z3")
summary(pr.out)
plot(pr.out)
pve <- 100 * pr.out$sdev^2 / sum(pr.out$sdev^2)
par(mfrow = c(1, 2))
plot(pve,  type = "o", ylab = "PVE",
xlab = "Principal Component", col = "blue")
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE",
xlab = "Principal Component", col = "brown3")
sd.data <- scale(nci.data)
par(mfrow = c(1, 3))
data.dist <- dist(sd.data)
plot(hclust(data.dist), xlab = "", sub = "", ylab = "",
labels = nci.labs, main = "Complete Linkage")
plot(hclust(data.dist, method = "average"),
labels = nci.labs, main = "Average Linkage",
xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"),
labels = nci.labs,  main = "Single Linkage",
xlab = "", sub = "", ylab = "")
hc.out <- hclust(dist(sd.data))
hc.clusters <- cutree(hc.out, 4)
table(hc.clusters, nci.labs)
par(mfrow = c(1, 1))
plot(hc.out, labels = nci.labs)
abline(h = 139, col = "red")
hc.out
set.seed(2)
km.out <- kmeans(sd.data, 4, nstart = 20)
km.clusters <- km.out$cluster
table(km.clusters, hc.clusters)
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, labels = nci.labs,
main = "Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out, 4), nci.labs)
# Lab: Multiple Testing
## Review of Hypothesis Tests
###
set.seed(6)
x <- matrix(rnorm(10 * 100), 10, 100)
x[, 1:50] <- x[, 1:50] + 0.5
###
t.test(x[, 1], mu = 0)
###
p.values <- rep(0, 100)
for (i in 1:100)
p.values[i] <- t.test(x[, i], mu = 0)$p.value
decision <- rep("Do not reject H0", 100)
decision[p.values <= .05] <- "Reject H0"
###
table(decision,
c(rep("H0 is False", 50), rep("H0 is True", 50))
)
###
###
x <- matrix(rnorm(10 * 100), 10, 100)
x[, 1:50] <- x[, 1:50] + 1
for (i in 1:100)
p.values[i] <- t.test(x[, i], mu = 0)$p.value
decision <- rep("Do not reject H0", 100)
decision[p.values <= .05] <- "Reject H0"
table(decision,
c(rep("H0 is False", 50), rep("H0 is True", 50))
)
## The Family-Wise Error Rate
###
m <- 1:500
fwe1 <- 1 - (1 - 0.05)^m
fwe2 <- 1 - (1 - 0.01)^m
fwe3 <- 1 - (1 - 0.001)^m
###
par(mfrow = c(1, 1))
plot(m, fwe1, type = "l", log = "x", ylim = c(0, 1), col = 2,
ylab = "Family - Wise Error Rate",
xlab = "Number of Hypotheses")
lines(m, fwe2, col = 4)
lines(m, fwe3, col = 3)
abline(h = 0.05, lty = 2)
###
library(ISLR2)
fund.mini <- Fund[, 1:5]
t.test(fund.mini[, 1], mu = 0)
fund.pvalue <- rep(0, 5)
for (i in 1:5)
fund.pvalue[i] <- t.test(fund.mini[, i], mu = 0)$p.value
fund.pvalue
###
p.adjust(fund.pvalue, method = "bonferroni")
pmin(fund.pvalue * 5, 1)
###
p.adjust(fund.pvalue, method = "holm")
###
apply(fund.mini, 2, mean)
###
t.test(fund.mini[, 1], fund.mini[, 2], paired = T)
###
returns <- as.vector(as.matrix(fund.mini))
manager <- rep(c("1", "2", "3", "4", "5"), rep(50, 5))
a1 <- aov(returns ~ manager)
TukeyHSD(x = a1)
###
plot(TukeyHSD(x = a1))
## The False Discovery Rate
###
fund.pvalues <- rep(0, 2000)
for (i in 1:2000)
fund.pvalues[i] <- t.test(Fund[, i], mu = 0)$p.value
###
q.values.BH <- p.adjust(fund.pvalues, method = "BH")
q.values.BH[1:10]
###
sum(q.values.BH <= .1)
###
sum(fund.pvalues <= (0.1 / 2000))
###
ps <- sort(fund.pvalues)
m <- length(fund.pvalues)
q <- 0.1
wh.ps <- which(ps < q * (1:m) / m)
if (length(wh.ps) >0) {
wh <- 1:max(wh.ps)
} else {
wh <- numeric(0)
}
###
plot(ps, log = "xy", ylim = c(4e-6, 1), ylab = "P-Value",
xlab = "Index", main = "")
points(wh, ps[wh], col = 4)
abline(a = 0, b = (q / m), col = 2, untf = TRUE)
abline(h = 0.1 / 2000, col = 3)
## A Re-Sampling Approach
###
attach(Khan)
x <- rbind(xtrain, xtest)
y <- c(as.numeric(ytrain), as.numeric(ytest))
dim(x)
table(y)
###
x <- as.matrix(x)
x1 <- x[which(y == 2), ]
x2 <- x[which(y == 4), ]
n1 <- nrow(x1)
n2 <- nrow(x2)
t.out <- t.test(x1[, 11], x2[, 11], var.equal = TRUE)
TT <- t.out$statistic
TT
t.out$p.value
###
set.seed(1)
B <- 10000
Tbs <- rep(NA, B)
for (b in 1:B) {
dat <- sample(c(x1[, 11], x2[, 11]))
Tbs[b] <- t.test(dat[1:n1], dat[(n1 + 1):(n1 + n2)],
var.equal = TRUE
)$statistic
}
mean((abs(Tbs) >= abs(TT)))
###
hist(Tbs, breaks = 100, xlim = c(-4.2, 4.2), main = "",
xlab = "Null Distribution of Test Statistic", col = 7)
lines(seq(-4.2, 4.2, len = 1000),
dt(seq(-4.2, 4.2, len = 1000),
df = (n1 + n2 - 2)
) * 1000, col = 2, lwd = 3)
abline(v = TT, col = 4, lwd = 2)
text(TT + 0.5, 350, paste("T = ", round(TT, 4), sep = ""),
col = 4)
###
m <- 100
set.seed(1)
index <- sample(ncol(x1), m)
Ts <- rep(NA, m)
Ts.star <- matrix(NA, ncol = m, nrow = B)
for (j in 1:m) {
k <- index[j]
Ts[j] <- t.test(x1[, k], x2[, k],
var.equal = TRUE
)$statistic
for (b in 1:B) {
dat <- sample(c(x1[, k], x2[, k]))
Ts.star[b, j] <- t.test(dat[1:n1],
dat[(n1 + 1):(n1 + n2)], var.equal = TRUE
)$statistic
}
}
###
cs <- sort(abs(Ts))
FDRs <- Rs <- Vs <- rep(NA, m)
for (j in 1:m) {
R <- sum(abs(Ts) >= cs[j])
V <- sum(abs(Ts.star) >= cs[j]) / B
Rs[j] <- R
Vs[j] <- V
FDRs[j] <- V / R
}
###
max(Rs[FDRs <= .1])
sort(index[abs(Ts) >= min(cs[FDRs < .1])])
max(Rs[FDRs <= .2])
sort(index[abs(Ts) >= min(cs[FDRs < .2])])
###
plot(Rs, FDRs, xlab = "Number of Rejections", type = "l",
ylab = "False Discovery Rate", col = 4, lwd = 3)
###
install.packages("languageserver")
install.packages("Rtools")
reticulate::repl_python()
install.packages("xtable")
install.packages("xtable")
install.packages("hdm") # a library for high-dimensional metrics
install.packages("glmnet") # for lasso CV
install.packages('IRkernel')
IRkernel::installspec()  # to register the kernel in the current R installation
#import car data
library(ISLR2)
#linear regression
lm.fit <- lm(mpg ~ horsepower, data = Auto)
# test significance of the coefficient
summary(lm.fit)$coef
# interpret
confint(lm.fit)
